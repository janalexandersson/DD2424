\documentclass{article}	
\usepackage[utf8]{inputenc} 		
\usepackage{amsmath} 

\usepackage{verbatim}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{lineno}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{systeme}
\usepackage{amsmath,amssymb,amsthm,mathrsfs,latexsym,tikz,url}
\usepackage{epigraph,graphicx}
\usepackage{titlesec} %For formatting sections
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{color}
\usepackage{todonotes}
\usepackage{caption}
\usepackage{float}
\usepackage{cite}
\usepackage[font=small,labelfont=bf]{caption}
\presetkeys%
    {todonotes}%
    {inline,backgroundcolor=yellow}{}

\graphicspath{ {./}{./figures/} {images/}}
\DeclareGraphicsExtensions{.png,.pdf}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\setlength{\parindent}{0.0cm}
\setlength{\parskip}{0.1cm}

% \renewcommand{\familydefault}{ptm} %New Century Schoolbook
%pnc =  New Century Schoolbook
%ppl = palatino
%ptm = times new roman



% \titleformat{<command>}[<shape>]{<format>}{<label>}{<sep>}{<before-code>}[<after-code>]

\titleformat{\section}{\normalfont\scshape}{\thesection.}{1em}{}
% \titlespacing{\section}{0pc}{1.5ex plus .1ex minus .2ex}{0pc}
% %
% \titleformat{\subsection}[runin]{\normalfont\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsection}{\normalfont\bfseries}{\thesubsection}{1em}{}
% \titlespacing{\subsection}{0pc}{1.5ex plus .1ex minus .2ex}{1pc}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%








% See http://texblog.org/2007/11/07/headerfooter-in-latex-with-fancyhdr/
\fancyhead[R]{Jan Alexandersson, \thepage/\pageref{LastPage}}





\begin{document}


\title{Assignment 1 Bonus Deep Learning}
\author{Jan Alexandersson}
\maketitle 


\section{Possible improvements}

Using the settings \textbf{lambda = 1, n\_epochs = 40, n\_batch = 100, eta = 0.001} we got test accuracy of 0.3778 
without any of the suggested improvements. We then implemented the following possible improvements:

\begin{itemize}
 \item Shuffle the order of your training examples at the beginning of every epoch.
 \item Decaying the learning rate with a facor of 0.9 after each epoch.
 \item Train for a longer time and use your validation set to make sure you donâ€™t overfit or to keep a record of the best model before you begin to overfit.
\end{itemize}

\begin{center}
\begin{tabular}{ |c|c|c|c| } 
\hline
  & Test accuracy \\
\hline
Without any improvements & 0.3778  \\
\hline
Shuffled order & 0.3602  \\ 
\hline
Decaying Eta & 0.3733  \\ 
\hline
Longer training & 0.3779  \\ 
\hline
\end{tabular}
\end{center}

We see in the table above that we only got some minimal improvement on our test accuracy when we trained for a longer time 
and we actually performed slightly worse with the other methods. However, this may just be random differences since we initialize 
the $W$ matirx and $b$ with random numbers. I also tried this with some other values of the parameters but got similar results. 
However, surprisingly, the test accuracy seemed to be decrase when shuffeling the order of my 
training examples before each epoch, which might indicate that the implementation of the shuffeling is incorrect. 


\section{SVM Loss}


We used the same parameters as in the non-bonus assignment and used for all runs \textbf{n\_batches = 100} and 
\textbf{n\_epochs = 40} and varied the values of \textbf{eta} and \textbf{lambda}. 
The results are presented in the table of the test accuracys below. 

\begin{center}
\begin{tabular}{ |c|c|c| } 
\hline
Parameters  &  Cross-entropy loss & SVM multi-class loss  \\
\hline
 Eta $= 0.1$, lambda $= 0$ & 0.2751 & 0.2896 \\
\hline
Eta $= 0.001$, lambda $= 0$  & 0.3838 &  0.3879 \\ 
\hline
Eta $= 0.001$, lambda $= 0.1$   & 0.3898 &  0.3918  \\ 
\hline
Eta $= 0.001$, lambda $= 1$  & 0.3771  &  0.3767  \\ 
\hline 
\end{tabular}
\end{center}

We can see that there are no large differences between using cross-entropy loss 
compared to SVM multi-class loss with regards to test accuracy.
For some parameters SVM multi-class loss outperformed cross-entropy loss, but in the last row of the table above we can 
see that the cross-entropy loss slightly outperformed SVM multi-class loss. However, the differences are so small that they 
can be regarded as random. 

Here are some code used in this exercise:


\begin{lstlisting}
def ComputeCostSVM(X, Y, W, b, lamb):
	N = X.shape[1]

	s = EvaluateClassifier(X, W, b)
	sc = s.T[np.arange(s.shape[1]), np.argmax(Y, axis=0)].T

	marg = np.maximum(0, s - np.asarray(sc) + 1)
	marg.T[np.arange(N), np.argmax(Y, axis=0)] = 0

	mcsvm_loss = Y.shape[0] * np.mean(np.sum(marg, axis=1))

	cost = 1/N * mcsvm_loss + 0.5 * lamb * np.sum(W**2)

	return cost, marg


def ComputeGradientsSVM(X, Y, W, b, lamb):

	N = X.shape[1]

	_, marg = ComputeCostSVM(X, Y, W, b , lamb)

	bi = marg
	bi[marg > 0] = 1
	bi_sum_rows = np.sum(bi, axis=0)

	bi.T[np.arange(N), np.argmax(Y, axis=0)] = -bi_sum_rows.T

	grad_W = np.dot(bi, X.T) / N + lamb * W

	grad_b = np.reshape(np.sum(bi, axis=1) / bi.shape[1], b.shape)
	return grad_W, grad_b
\end{lstlisting}



\end{document}