\documentclass{article}	
\usepackage[utf8]{inputenc} 		
\usepackage{amsmath} 

\usepackage{verbatim}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{lineno}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{systeme}
\usepackage{amsmath,amssymb,amsthm,mathrsfs,latexsym,tikz,url}
\usepackage{epigraph,graphicx}
\usepackage{titlesec} %For formatting sections
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{color}
\usepackage{todonotes}
\usepackage{caption}
\usepackage{float}
\usepackage{cite}
\usepackage[font=small,labelfont=bf]{caption}
\presetkeys%
    {todonotes}%
    {inline,backgroundcolor=yellow}{}

\graphicspath{ {./}{./figures/} {images/}}
\DeclareGraphicsExtensions{.png,.pdf}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\setlength{\parindent}{0.0cm}
\setlength{\parskip}{0.1cm}

% \renewcommand{\familydefault}{ptm} %New Century Schoolbook
%pnc =  New Century Schoolbook
%ppl = palatino
%ptm = times new roman



% \titleformat{<command>}[<shape>]{<format>}{<label>}{<sep>}{<before-code>}[<after-code>]

\titleformat{\section}{\normalfont\scshape}{\thesection.}{1em}{}
% \titlespacing{\section}{0pc}{1.5ex plus .1ex minus .2ex}{0pc}
% %
% \titleformat{\subsection}[runin]{\normalfont\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsection}{\normalfont\bfseries}{\thesubsection}{1em}{}
% \titlespacing{\subsection}{0pc}{1.5ex plus .1ex minus .2ex}{1pc}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%








% See http://texblog.org/2007/11/07/headerfooter-in-latex-with-fancyhdr/
\fancyhead[R]{Jan Alexandersson, \thepage/\pageref{LastPage}}





\begin{document}


\title{Assignment 1 Deep Learning}
\author{Jan Alexandersson}
\maketitle 


\section{Check gradient}

\begin{lstlisting}
def ComputeGradients(X,Y,W,b, lamb):	
	P = EvaluateClassifier(X, W, b)
	g = -(Y-P)
	
	grad_W = np.matmul(g, X.T) / X.shape[1] + 2*lamb*W
	grad_b = np.matmul(g, np.ones((X.shape[1], 1))) / X.shape[1]
	
	return grad_W, grad_b
\end{lstlisting}


I checked the relative error using 

\begin{equation}
 \varepsilon = \max (|g_a - g_n| ./ \max(e , |g_a + g_n|)),
\end{equation}

where $e = 1e-6$. And the result was 



\begin{center}
\begin{tabular}{ |c|c|c|c| } 
\hline
  & Test accuracy \\
\hline
Without possible improvement & 0.3778  \\
Random Permutation & 0.3602  \\ 
Decaying Eta & 0.3733  \\ 
Longer training & 0.3779  \\ 
\hline
\end{tabular}
\end{center}

2.27091782e-07]
 [2.25096758e-07]
 [4.84234670e-07]
 [2.54172401e-06]
 [3.72547140e-06]
 [2.29920748e-07]
 [7.23912134e-07]
 [4.70193738e-07]
 [2.77355470e-06]
 [9.08686966e-08]]


\section{Results}


Using the settings \textbf{lambda = 0, n\_epochs = 40, n\_batch = 100, eta = 0.1} we got

\begin{itemize}
 \item Accuracy on training set: 0.4174
 \item Accuracy on test set: 0.2843
\end{itemize}


\begin{figure}[H]
\centering
  \includegraphics[width = 12cm]{../firstCost.pdf}
\caption{Cost function using \textbf{lambda = 0, n\_epochs = 40, n\_batch = 100, eta = 0.1}. }
\label{fig:firstCost}
\end{figure}

\begin{figure}[H]
\centering
  \includegraphics[width = 12cm]{../firstW.pdf}
\caption{Images representing the W matrix using settings \textbf{lambda = 0, n\_epochs = 40, n\_batch = 100, eta = 0.1}.}
\label{fig:firstW}
\end{figure}


Using the settings \textbf{lambda = 0, n\_epochs = 40, n\_batch = 100, eta = 0.1} we got

\begin{itemize}
 \item Accuracy on training set: 0.4558
 \item Accuracy on test set: 0.3889
\end{itemize}


\begin{figure}[H]
\centering
  \includegraphics[width = 12cm]{../secondCost.pdf}
\caption{Cost function using \textbf{lambda = 0, n\_epochs = 40, n\_batch = 100, eta = 0.001}. }
\label{fig:secondCost}
\end{figure}

\begin{figure}[H]
\centering
  \includegraphics[width = 12cm]{../secondW.pdf}
\caption{Images representing the W matrix using settings \textbf{lambda = 0, n\_epochs = 40, n\_batch = 100, eta = 0.001}.}
\label{fig:secondW}
\end{figure}


Using the settings \textbf{lambda = 0.1, n\_epochs = 40, n\_batch = 100, eta = 0.001} we got

\begin{itemize}
 \item Accuracy on training set: 0.4471
 \item Accuracy on test set: 0.3924
\end{itemize}

\begin{figure}[H]
\centering
  \includegraphics[width = 12cm]{../thirdCost.pdf}
\caption{Cost function using \textbf{lambda = 0.1, n\_epochs = 40, n\_batch = 100, eta = 0.001}. }
\label{fig:thirdCost}
\end{figure}

\begin{figure}[H]
\centering
  \includegraphics[width = 12cm]{../thirdW.pdf}
\caption{Images representing the W matrix using settings \textbf{lambda = 0, n\_epochs = 40, n\_batch = 100, eta = 0.001}.}
\label{fig:thirdW}
\end{figure}

Using the settings \textbf{lambda = 0.1, n\_epochs = 40, n\_batch = 100, eta = 0.001} we got

\begin{itemize}
 \item Accuracy on training set: 0.4016
 \item Accuracy on test set: 0.3776
\end{itemize}

\begin{figure}[H]
\centering
  \includegraphics[width = 12cm]{../fourthCost.pdf}
\caption{Cost function using \textbf{lambda = 1, n\_epochs = 40, n\_batch = 100, eta = 0.001}. }
\label{fig:fourthCost}
\end{figure}

\begin{figure}[H]
\centering
  \includegraphics[width = 12cm]{../fourthW.pdf}
\caption{Images representing the W matrix using settings \textbf{lambda = 1, n\_epochs = 40, n\_batch = 100, eta = 0.001}.}
\label{fig:fourthW}
\end{figure}


When increasing the learning rate the cost function converge faster but is will oscillate be more 
unstable, which we can see in Figure 1. The learning rate is too large and we will jump around the minumum 
and not stay at the minimum when we perform the minibatch gradient decent. 
A lower learning rate will cause the cost function to converge slower but the curve will be more stable and our results 
are more reliable. 

When increasing the regularisation term we get a smoother representation of the W matrix, which we see in Figure 8. 
This is because when increasing the regularisation term we restrict the model and will generalize more, however a too 
large value of the regularisation term will generalize too much and reulst in dropped test accuracy while a too low value 
will cause overfitting. 





\end{document}