\documentclass{article}	
\usepackage[utf8]{inputenc} 		
\usepackage{amsmath} 

\usepackage{verbatim}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{lineno}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{systeme}
\usepackage{amsmath,amssymb,amsthm,mathrsfs,latexsym,tikz,url}
\usepackage{epigraph,graphicx}
\usepackage{titlesec} %For formatting sections
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{color}
\usepackage{todonotes}
\usepackage{caption}
\usepackage{float}
\usepackage{cite}
\usepackage[font=small,labelfont=bf]{caption}
\presetkeys%
    {todonotes}%
    {inline,backgroundcolor=yellow}{}

\graphicspath{ {./}{./figures/} {images/}}
\DeclareGraphicsExtensions{.png,.pdf}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\setlength{\parindent}{0.0cm}
\setlength{\parskip}{0.1cm}

% \renewcommand{\familydefault}{ptm} %New Century Schoolbook
%pnc =  New Century Schoolbook
%ppl = palatino
%ptm = times new roman



% \titleformat{<command>}[<shape>]{<format>}{<label>}{<sep>}{<before-code>}[<after-code>]

\titleformat{\section}{\normalfont\scshape}{\thesection.}{1em}{}
% \titlespacing{\section}{0pc}{1.5ex plus .1ex minus .2ex}{0pc}
% %
% \titleformat{\subsection}[runin]{\normalfont\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsection}{\normalfont\bfseries}{\thesubsection}{1em}{}
% \titlespacing{\subsection}{0pc}{1.5ex plus .1ex minus .2ex}{1pc}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%








% See http://texblog.org/2007/11/07/headerfooter-in-latex-with-fancyhdr/
\fancyhead[R]{Jan Alexandersson, \thepage/\pageref{LastPage}}





\begin{document}


\title{Assignment 1 Deep Learning}
\author{Jan Alexandersson}
\maketitle 


\section{Possible improvements}

Using the settings \textbf{lambda = 1, n\_epochs = 40, n\_batch = 100, eta = 0.001} we got test accuracy of 0.3778 
without any of the suggested improvements. We then implemented the following possible improvements:

\begin{itemize}
 \item Shuffle the order of your training examples at the beginning of every epoch.
 \item Decaying the learning rate with a facor of 0.9 after each epoch.
 \item Train for a longer time and use your validation set to make sure you donâ€™t overfit or to keep a record of the best model before you begin to overfit.
\end{itemize}

\begin{center}
\begin{tabular}{ |c|c|c|c| } 
\hline
  & Test accuracy \\
\hline
Without any improvements & 0.3778  \\
\hline
Shuffled order & 0.3602  \\ 
\hline
Decaying Eta & 0.3733  \\ 
\hline
Longer training & 0.3779  \\ 
\hline
\end{tabular}
\end{center}

We see in the table above that we only got some minimal improvement on our test accuracy when we trained for a longer time 
and we actually performed slightly worse with the other methods. However, this may just be random differences since we initialize 
the $W$ matirx and $b$ with random numbers. I also tried this with some other values of the parameters but got similar results. 
However, surprisingly, the test accuracy seemed to be decrase when shuffeling the order of my 
training examples before each epoch, which might indicate that the implementation of the shuffeling is incorrect. 


\section{SVM Loss}


We used the same parameters as in the non-bonus assignment and used for all runs \textbf{n\_batches = 100} and 
\textbf{n\_epochs = 40} and varied the values of \textbf{eta} and \textbf{lambda}. The results are presented in the table below. 

\begin{center}
\begin{tabular}{ |c|c|c| } 
\hline
Parameters  &  Cross-entropy loss & SVM loss  \\
\hline
 Eta $= 0.1$, lambda $= 0$ & 0.282 & 0.2727 \\
\hline
Eta $= 0.01$, lambda $= 0$  & 0.3595 &  0.3571 \\ 
\hline
Eta $= 0.01$, lambda $= 0.1$   & 0.3662 &  0.3662  \\ 
\hline
Eta $= 0.01$, lambda $= 1$  & 0.3357  &  0.3356  \\ 
\hline
\end{tabular}
\end{center}


We can see that the cross-entropy loss slightly outperformed the SVM multi-class loss with regards to test accuracy


\end{document}